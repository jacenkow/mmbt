{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Generate Results\n",
    "\n",
    "The main evaluation matrics to report performance of the tested methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    multilabel_confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    0: \"Atelectasis\",\n",
    "    1: \"Cardiomegaly\",\n",
    "    2: \"Consolidation\",\n",
    "    3: \"Edema\",\n",
    "    4: \"Enlarged Cardiomediastinum\",\n",
    "    5: \"Fracture\",\n",
    "    6: \"Lung Lesion\",\n",
    "    7: \"Lung Opacity\",\n",
    "    8: \"No Finding\",\n",
    "    9: \"Pleural Effusion\",\n",
    "    10: \"Pleural Other\",\n",
    "    11: \"Pneumonia\",\n",
    "    12: \"Pneumothorax\",\n",
    "    13: \"Support Devices\",\n",
    "}\n",
    "\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabelConfussionMatrix(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Returns the TP, FP, TN, FN\n",
    "    \"\"\"\n",
    "    TP = np.zeros(y_test.shape[1])\n",
    "    FP = np.zeros(y_test.shape[1])\n",
    "    TN = np.zeros(y_test.shape[1])\n",
    "    FN = np.zeros(y_test.shape[1])\n",
    "\n",
    "    for j in range(y_test.shape[1]):\n",
    "        TPaux = 0\n",
    "        FPaux = 0\n",
    "        TNaux = 0\n",
    "        FNaux = 0\n",
    "        for i in range(y_test.shape[0]):\n",
    "            if int(y_test[i,j]) == 1:\n",
    "                if int(y_test[i,j]) == 1 and int(predictions[i,j]) == 1:\n",
    "                    TPaux += 1\n",
    "                else:\n",
    "                    FPaux += 1\n",
    "            else:\n",
    "                if int(y_test[i,j]) == 0 and int(predictions[i,j]) == 0:\n",
    "                    TNaux += 1\n",
    "                else:\n",
    "                    FNaux += 1\n",
    "        TP[j] = TPaux\n",
    "        FP[j] = FPaux\n",
    "        TN[j] = TNaux\n",
    "        FN[j] = FNaux\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def multilabelMicroConfussionMatrix(TP, FP, TN, FN):\n",
    "    TPMicro = 0.0\n",
    "    FPMicro = 0.0\n",
    "    TNMicro = 0.0\n",
    "    FNMicro = 0.0\n",
    "    \n",
    "    for i in range(len(TP)):\n",
    "        TPMicro = TPMicro + TP[i]\n",
    "        FPMicro = FPMicro + FP[i]\n",
    "        TNMicro = TNMicro + TN[i]\n",
    "        FNMicro = FNMicro + FN[i]\n",
    "    \n",
    "    return TPMicro, FPMicro, TNMicro, FNMicro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(y_test, predictions):\n",
    "    accuracy = []\n",
    "    for i in range(14):\n",
    "        accuracy.append(\n",
    "            accuracy_score(y_test[i], predictions[i])\n",
    "        )\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def accuracyMacro(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Accuracy Macro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    accuracymacro : float\n",
    "        Accuracy Macro of our model\n",
    "    \"\"\"\n",
    "    accuracymacro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    for i in range(len(TP)):\n",
    "        accuracymacro += ((TP[i] + TN[i])/(TP[i] + FP[i] + TN[i] + FN[i]))\n",
    "    \n",
    "    accuracymacro = float(accuracymacro/len(TP))\n",
    "\n",
    "    return accuracymacro\n",
    "\n",
    "\n",
    "def precisionMacro(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Precision Macro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    precisionmacro : float\n",
    "        Precision macro of our model\n",
    "    \"\"\"\n",
    "    precisionmacro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    for i in range(len(TP)):\n",
    "        if TP[i] + FP[i] != 0:\n",
    "            precisionmacro = precisionmacro + (TP[i]/(TP[i] + FP[i]))\n",
    "\n",
    "    precisionmacro = float(precisionmacro/len(TP))\n",
    "    return precisionmacro\n",
    "\n",
    "\n",
    "def precisionMicro(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Precision Micro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    precisionmicro : float\n",
    "        Precision micro of our model\n",
    "    \"\"\"\n",
    "    precisionmicro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    TPMicro, FPMicro, TNMicro, FNMicro =\\\n",
    "        multilabelMicroConfussionMatrix(TP, FP, TN, FN)\n",
    "    if (TPMicro + FPMicro) != 0:\n",
    "        precisionmicro = float(TPMicro/(TPMicro + FPMicro))\n",
    "\n",
    "\n",
    "    return precisionmicro\n",
    "\n",
    "\n",
    "def recallMacro(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Recall Macro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    recallmacro : float\n",
    "        Recall Macro of our model\n",
    "    \"\"\"\n",
    "    recallmacro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    for i in range(len(TP)):\n",
    "        if TP[i] + FN[i] != 0:\n",
    "            recallmacro = recallmacro + (TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    recallmacro = recallmacro/len(TP)\n",
    "    return recallmacro\n",
    "\n",
    "\n",
    "def recallMicro(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Recall Micro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    recallmicro : float\n",
    "        Recall Micro of our model\n",
    "    \"\"\"\n",
    "    recallmicro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    TPMicro, FPMicro, TNMicro, FNMicro =\\\n",
    "        multilabelMicroConfussionMatrix(TP, FP, TN, FN)\n",
    "\n",
    "    if (TPMicro + FNMicro) != 0:\n",
    "        recallmicro = float(TPMicro/(TPMicro + FNMicro))\n",
    "\n",
    "    return recallmicro\n",
    "\n",
    "\n",
    "def fbetaMacro(y_test, predictions, beta=1):\n",
    "    \"\"\"\n",
    "    FBeta Macro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    fbetamacro : float\n",
    "        FBeta Macro of our model\n",
    "    \"\"\"\n",
    "    fbetamacro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "\n",
    "    for i in range(len(TP)):\n",
    "        num = float((1+pow(beta,2))*TP[i])\n",
    "        den = float((1+pow(beta,2))*TP[i] + pow(beta,2)*FN[i] + FP[i])\n",
    "        if den != 0:\n",
    "            fbetamacro = fbetamacro + num/den\n",
    "\n",
    "    fbetamacro = fbetamacro/len(TP)\n",
    "    return fbetamacro\n",
    "\n",
    "\n",
    "def fbetaMicro(y_test, predictions, beta=1):\n",
    "    \"\"\"\n",
    "    FBeta Micro of our model\n",
    "    Params\n",
    "    ======\n",
    "    y_test : sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of labels used in the test phase\n",
    "    predictions: sparse or dense matrix (n_samples, n_labels)\n",
    "        Matrix of predicted labels given by our model\n",
    "    Returns\n",
    "    =======\n",
    "    fbetamicro : float\n",
    "        FBeta Micro of our model\n",
    "    \"\"\"\n",
    "    fbetamicro = 0.0\n",
    "    TP, FP, TN, FN = multilabelConfussionMatrix(y_test, predictions)\n",
    "    TPMicro, FPMicro, TNMicro, FNMicro =\\\n",
    "        multilabelMicroConfussionMatrix(TP, FP, TN, FN)\n",
    "\n",
    "    num = float((1+pow(beta,2))*TPMicro)\n",
    "    den = float((1+pow(beta,2))*TPMicro + pow(beta,2)*FNMicro + FPMicro)\n",
    "    fbetamicro = float(num/den)\n",
    "\n",
    "    return fbetamicro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc(ground_truth, predictions):\n",
    "    # Micro.\n",
    "    fpr, tpr, _ = roc_curve(ground_truth.ravel(), predictions.ravel())\n",
    "    micro_roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Macro.\n",
    "    macro_roc_auc = 0\n",
    "    for i in range(14):\n",
    "        fpr, tpr, _ = roc_curve(ground_truth[:, i], predictions[:, i])\n",
    "        macro_roc_auc += (auc(fpr, tpr))\n",
    "\n",
    "    macro_roc_auc /= 14\n",
    "\n",
    "    return macro_roc_auc, micro_roc_auc\n",
    "    \n",
    "\n",
    "def print_stats(ground_truth, predictions):\n",
    "    thresholded = (np.array(predictions) > THRESHOLD).astype(int)\n",
    "\n",
    "    print(\"Accuracy:\" + str(np.round(accuracyMacro(ground_truth, thresholded), 3)))\n",
    "    print(\"Precision Macro:\" + str(np.round(precisionMacro(ground_truth, thresholded), 3)))\n",
    "    print(\"Precision Micro:\" + str(np.round(precisionMicro(ground_truth, thresholded), 3)))\n",
    "    print(\"Recall Macro:\" + str(np.round(recallMacro(ground_truth, thresholded), 3)))\n",
    "    print(\"Recall Micro:\" + str(np.round(recallMicro(ground_truth, thresholded), 3)))\n",
    "    print(\"F1 Macro:\" + str(np.round(fbetaMacro(ground_truth, thresholded), 3)))\n",
    "    print(\"F1 Micro:\" + str(np.round(fbetaMicro(ground_truth, thresholded), 3)))\n",
    "\n",
    "    macro_roc_auc, micro_roc_auc = calc_roc(ground_truth, np.array(predictions))\n",
    "    print(\"AUROC Macro:\" + str(np.round(np.average(macro_roc_auc), 3)))\n",
    "    print(\"AUROC Micro:\" + str(np.round(micro_roc_auc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"../training\"\n",
    "\n",
    "for i in glob(os.path.join(PREFIX_A, \"*\", \"*\", \"reports\", \"*.json\")):\n",
    "    print(i.split(\"/\")[-4])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    with open(i, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    predictions = [i['probabilities'] for i in results]\n",
    "    ground_truth = np.array([i['ground_truth'] for i in results])\n",
    "\n",
    "    print_stats(ground_truth, predictions)\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
