{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing - Adversarial Attacks\n",
    "\n",
    "We test the model to textual changes with an evaluation scheme proposed in\n",
    "*On Adversarial Examples for Biomedical NLP Tasks* by (Araujo et al., 2020),\n",
    "which we further extended. We mimic a human operator who commits typographical\n",
    "errors and expresses the original medical terms with synonyms. We only select\n",
    "biomedical terms to proceed with the following word/sentence manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS = \"../../datasets/mimic_cxr/annotations/\"\n",
    "\n",
    "TUI_WHITELIST = [\n",
    "    \"3300\", \"T005\", \"T007\", \"T017\", \"T019\", \"T020\", \"T021\", \"T022\", \"T023\",\n",
    "    \"T024\", \"T025\", \"T029\", \"T030\", \"T033\", \"T034\", \"T037\", \"T039\", \"T041\",\n",
    "    \"T046\", \"T047\", \"T048\", \"T055\", \"T056\", \"T059\", \"T060\", \"T060\", \"T061\",\n",
    "    \"T074\", \"T093\", \"T097\", \"T101\", \"T121\", \"T125\", \"T170\", \"T184\", \"T190\",\n",
    "    \"T191\", \"T195\", \"T201\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scispaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pipeline = spacy.load(\"en_core_sci_sm\")\n",
    "spacy_pipeline.add_pipe(\n",
    "    \"scispacy_linker\",\n",
    "    config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "linker = spacy_pipeline.get_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANNOTATIONS, \"test.jsonl\"), \"r\") as f:\n",
    "    test = [json.loads(i) for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_letters(text):\n",
    "    # Tokenise.\n",
    "    words = [i for i in text.split(\" \")]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if len(words[i]) < 2:\n",
    "            continue \n",
    "\n",
    "        # Swap two characters for each word.\n",
    "        word = list(words[i])\n",
    "        j = random.randint(0, len(word) - 2)\n",
    "        word[j], word[j + 1] = word[j + 1], word[j]\n",
    "        words[i] = \"\".join(word)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misplaced_letters(text):\n",
    "    _keys = {\"A\": [\"Q\", \"Z\", \"W\", \"S\"],\n",
    "             \"B\": [\"G\", \"V\", \"N\", \"H\"],\n",
    "             \"C\": [\"D\", \"X\", \"F\", \"V\"],\n",
    "             \"D\": [\"E\", \"S\", \"X\", \"C\", \"F\", \"R\"],\n",
    "             \"E\": [\"3\", \"4\", \"W\", \"S\", \"D\", \"R\"],\n",
    "             \"F\": [\"R\", \"D\", \"C\", \"V\", \"G\", \"T\"],\n",
    "             \"G\": [\"T\", \"F\", \"V\", \"B\", \"H\", \"Y\"],\n",
    "             \"H\": [\"Y\", \"G\", \"B\", \"N\", \"J\", \"U\"],\n",
    "             \"I\": [\"8\", \"9\", \"U\", \"J\", \"K\", \"O\"],\n",
    "             \"J\": [\"U\", \"H\", \"N\", \"M\", \"K\", \"I\"],\n",
    "             \"K\": [\"I\", \"J\", \"M\", \"L\", \"O\"],\n",
    "             \"L\": [\"O\", \"K\", \"P\"],\n",
    "             \"M\": [\"J\", \"N\", \"K\"],\n",
    "             \"N\": [\"H\", \"B\", \"M\", \"J\"],\n",
    "             \"O\": [\"9\", \"0\", \"I\", \"K\", \"L\", \"P\"],\n",
    "             \"P\": [\"O\", \"L\"],\n",
    "             \"Q\": [\"1\", \"2\", \"W\", \"A\"],\n",
    "             \"R\": [\"4\", \"5\", \"E\", \"D\", \"F\", \"T\"],\n",
    "             \"S\": [\"W\", \"A\", \"Z\", \"X\", \"D\", \"E\"],\n",
    "             \"T\": [\"5\", \"6\", \"R\", \"F\", \"G\", \"Y\"],\n",
    "             \"U\": [\"7\", \"8\", \"Y\", \"H\", \"J\", \"I\"],\n",
    "             \"V\": [\"F\", \"C\", \"B\", \"G\"],\n",
    "             \"W\": [\"2\", \"3\", \"Q\", \"A\", \"S\", \"E\"],\n",
    "             \"X\": [\"S\", \"Z\", \"D\", \"C\"],\n",
    "             \"Y\": [\"6\", \"7\", \"T\", \"G\", \"H\", \"U\"],\n",
    "             \"Z\": [\"A\", \"S\", \"X\"]}\n",
    "    \n",
    "    # Tokenise.\n",
    "    words = [i for i in text.split(\" \")]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if len(words[i]) < 2:\n",
    "            continue\n",
    "\n",
    "        word = list(words[i])\n",
    "        \n",
    "        is_lower = False\n",
    "        j = random.randint(0, len(word) - 1)\n",
    "        \n",
    "        if word[j].islower():\n",
    "            is_lower = True\n",
    "            \n",
    "        try:  # swap a random character with an adjacent one.\n",
    "            word[j] = random.choice(_keys[word[j].upper()])\n",
    "            \n",
    "            if is_lower:\n",
    "                word[j] = word[j].lower()\n",
    "        except KeyError:  # might be a special character.\n",
    "            continue\n",
    "            \n",
    "        words[i] = \"\".join(word)\n",
    "        \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANNOTATIONS, \"test.jsonl\"), \"r\") as f:\n",
    "    test = [json.loads(i) for i in f.readlines()]\n",
    "\n",
    "for i in tqdm(test):\n",
    "    pre_text = spacy_pipeline(i['text'])\n",
    "    post_text = deepcopy(i['text'])\n",
    "    umls_match = set()\n",
    "\n",
    "    # Match entities with UMLS.\n",
    "    for entity in pre_text.ents:\n",
    "        for umls_entity in entity._.kb_ents:\n",
    "            if linker.kb.cui_to_entity[umls_entity[0]].types[0] in TUI_WHITELIST:\n",
    "                umls_match.add(entity)\n",
    "\n",
    "    # Augmentation with swapping.\n",
    "    for entity in list(umls_match):\n",
    "        post_text = post_text.replace(str(entity), swap_letters(str(entity)))\n",
    "\n",
    "    i['text'] = post_text\n",
    "\n",
    "# with open(os.path.join(ANNOTATIONS, \"test_attack_swapping.jsonl\"), \"w\") as f:\n",
    "#     for sample in tqdm(test):\n",
    "#         f.write(json.dumps(sample) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms-based Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANNOTATIONS, \"test.jsonl\"), \"r\") as f:\n",
    "    test = [json.loads(i) for i in f.readlines()]\n",
    "\n",
    "for i in tqdm(test):\n",
    "    pre_text = spacy_pipeline(i['text'])\n",
    "    post_text = deepcopy(i['text'])\n",
    "    umls_match = {}\n",
    "\n",
    "    # Match entities with UMLS.\n",
    "    for entity in pre_text.ents:\n",
    "        for umls_entity in entity._.kb_ents:\n",
    "            if linker.kb.cui_to_entity[umls_entity[0]].types[0] in TUI_WHITELIST:\n",
    "                try:\n",
    "                    umls_match[str(entity)] = random.choice(\n",
    "                        linker.kb.cui_to_entity[umls_entity[0]].aliases)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "    # Augmentation with synonyms.\n",
    "    for entity in umls_match.keys():\n",
    "        post_text = post_text.replace(str(entity), umls_match[str(entity)])\n",
    "\n",
    "    i['text'] = post_text\n",
    "    \n",
    "\n",
    "# with open(os.path.join(ANNOTATIONS, \"test_attack_synonyms.jsonl\"), \"w\") as f:\n",
    "#     for sample in tqdm(test):\n",
    "#         f.write(json.dumps(sample) + \"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacement-based Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANNOTATIONS, \"test.jsonl\"), \"r\") as f:\n",
    "    test = [json.loads(i) for i in f.readlines()]\n",
    "\n",
    "test_healthy = [i for i in test if np.sum(\n",
    "    i['labels']) == 1 and np.argmax(i['labels']) == 8]\n",
    "test_rest = [i for i in test if not i in test_healthy]\n",
    "\n",
    "for i in tqdm(test_healthy):\n",
    "    post_text = deepcopy(i['text'])\n",
    "    \n",
    "    # Randomly select an indication field from the `unhealthy` patients.\n",
    "    post_text = random.choice(test_rest)['text']\n",
    "    \n",
    "    i['text'] = post_text\n",
    "\n",
    "# with open(os.path.join(ANNOTATIONS, \"test_attack_replacement.jsonl\"), \"w\") as f:\n",
    "#     for sample in tqdm(test_healthy):\n",
    "#         f.write(json.dumps(sample) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANNOTATIONS, \"test.jsonl\"), \"r\") as f:\n",
    "    test = [json.loads(i) for i in f.readlines()]\n",
    "\n",
    "test_map = {}\n",
    "    \n",
    "for i in test:\n",
    "    test_map[i['id']] = []\n",
    "    \n",
    "    for j in test:\n",
    "        overlap = False\n",
    "\n",
    "        if i == j:\n",
    "            continue\n",
    "            \n",
    "        for k in range(14):\n",
    "            if i['labels'][k] == 1 and j['labels'][k] == 1:\n",
    "                overlap = True\n",
    "                break\n",
    "                \n",
    "        if not overlap:\n",
    "            test_map[i['id']].append(j['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shuffled = []\n",
    "\n",
    "for i in test:\n",
    "    subject = deepcopy(i)\n",
    "    random_subject_id = random.choice(test_map[subject['id']])\n",
    "    \n",
    "    for j in test:\n",
    "        if random_subject_id == j['id']:\n",
    "            subject['text'] = j['text']\n",
    "    \n",
    "    test_shuffled.append(subject)\n",
    "    \n",
    "# with open(os.path.join(ANNOTATIONS,\n",
    "#                        \"test_attack_multilabel_replacement.jsonl\"), \"w\") as f:\n",
    "#     for sample in tqdm(test_shuffled):\n",
    "#         f.write(json.dumps(sample) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}